当前的args.activity_id:1
还原的分组结果: {0: [2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 22.0, 23.0, 24.0, 25.0, 26.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 47.0, 48.0, 49.0, 50.0, 51.0, 53.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 79.0, 82.0, 83.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 98.0, 99.0, 100.0, 102.0, 105.0, 107.0, 109.0, 113.0, 115.0, 116.0, 119.0, 120.0, 122.0, 125.0, 127.0, 128.0], 1: [103.0, 104.0, 106.0, 110.0, 111.0, 112.0, 114.0, 117.0, 118.0, 123.0, 124.0, 126.0, 129.0, 130.0, 131.0, 133.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 149.0, 150.0, 151.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 161.0, 162.0, 163.0, 164.0, 166.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 194.0, 195.0, 197.0, 198.0, 201.0, 202.0, 206.0, 208.0, 210.0, 211.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 223.0, 226.0, 228.0, 233.0, 234.0, 235.0, 236.0, 237.0, 239.0, 240.0, 242.0, 243.0, 244.0, 245.0, 246.0, 248.0], 2: [196.0, 200.0, 207.0, 209.0, 213.0, 222.0, 224.0, 225.0, 238.0, 241.0, 249.0, 252.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 309.0, 310.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 322.0, 323.0, 324.0, 325.0, 327.0, 328.0, 331.0, 332.0, 333.0, 336.0, 338.0, 339.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0], 3: [321.0, 326.0, 330.0, 334.0, 340.0, 350.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 390.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 403.0, 404.0, 405.0, 406.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 417.0, 418.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 445.0, 446.0, 447.0, 448.0, 450.0, 451.0, 452.0, 453.0, 454.0, 456.0, 458.0, 460.0, 461.0, 462.0, 464.0, 465.0, 467.0, 468.0, 469.0]}
Environment:
	Python: 3.10.9
	PyTorch: 1.13.1
	Torchvision: 0.14.1
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.23.5
	PIL: 10.4.0
==========================================
algorithm:diversify
alpha:0.1
alpha1:1.0
batch_size:1152
beta1:0.5
bottleneck:256
checkpoint_freq:100
classifier:linear
data_file:
dataset:pads
data_dir:./data/
dis_hidden:256
gpu_id:0
layer:bn
lam:0.0
latent_domain_num:2
local_epoch:10
lr:0.01
lr_decay1:1.0
lr_decay2:1.0
max_epoch:100
model_size:median
N_WORKERS:4
old:False
seed:0
task:cross_people
test_envs:[0]
output:./data/train_output/act/cross_people-our-Diversify-0-10-1-1-0-3-50-0.01
weight_decay:0.0005
activity_id:1
steps_per_epoch:10000000000
select_position:{'pads': [0]}
select_channel:{'pads': array([0, 1, 2, 3, 4, 5])}
hz_list:{'pads': 100}
act_people:{'pads': [[2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 22.0, 23.0, 24.0, 25.0, 26.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 47.0, 48.0, 49.0, 50.0, 51.0, 53.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 79.0, 82.0, 83.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 98.0, 99.0, 100.0, 102.0, 105.0, 107.0, 109.0, 113.0, 115.0, 116.0, 119.0, 120.0, 122.0, 125.0, 127.0, 128.0], [103.0, 104.0, 106.0, 110.0, 111.0, 112.0, 114.0, 117.0, 118.0, 123.0, 124.0, 126.0, 129.0, 130.0, 131.0, 133.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 149.0, 150.0, 151.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 161.0, 162.0, 163.0, 164.0, 166.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 194.0, 195.0, 197.0, 198.0, 201.0, 202.0, 206.0, 208.0, 210.0, 211.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 223.0, 226.0, 228.0, 233.0, 234.0, 235.0, 236.0, 237.0, 239.0, 240.0, 242.0, 243.0, 244.0, 245.0, 246.0, 248.0], [196.0, 200.0, 207.0, 209.0, 213.0, 222.0, 224.0, 225.0, 238.0, 241.0, 249.0, 252.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 309.0, 310.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 322.0, 323.0, 324.0, 325.0, 327.0, 328.0, 331.0, 332.0, 333.0, 336.0, 338.0, 339.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0], [321.0, 326.0, 330.0, 334.0, 340.0, 350.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 390.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 403.0, 404.0, 405.0, 406.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 417.0, 418.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 445.0, 446.0, 447.0, 448.0, 450.0, 451.0, 452.0, 453.0, 454.0, 456.0, 458.0, 460.0, 461.0, 462.0, 464.0, 465.0, 467.0, 468.0, 469.0]]}
num_classes:2
input_shape:(6, 1, 100)
grid_size:10


========ROUND 0========
====Feature update====
epoch            class_loss      
0                1.4057345390    
1                1.0655423403    
2                0.9486770630    
3                0.6605066657    
4                0.5273874402    
5                0.6355366707    
6                0.4503616989    
7                0.4785713255    
8                0.4503014982    
9                0.4406233430    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                1.1305075884     0.4906949699     0.6398125887    
1                0.8935236335     0.4803805351     0.4131430984    
2                0.7795544267     0.5640267134     0.2155277133    
3                0.6258750558     0.5437644124     0.0821106285    
4                0.6805156469     0.6468704939     0.0336451530    
5                0.6499156952     0.6330817342     0.0168339610    
6                0.5365858674     0.5270144343     0.0095714470    
7                0.5661242604     0.5588772297     0.0072470447    
8                0.6009868383     0.5968801379     0.0041067000    
9                0.5314094424     0.5288978815     0.0025115449    
Counter({0: 3081, 1: 2175})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.8741043210     0.3334515989     1.2075558901     0.7465753425     0.7465753425     0.4693877551     0.2516350746    
1                0.4418708384     0.2407872975     0.6826581359     0.8184931507     0.8184931507     0.6428571429     0.4359714985    
2                0.3595815301     0.2634386122     0.6230201721     0.8630136986     0.8630136986     0.6530612245     0.6227765083    
3                0.4453960955     0.3843193054     0.8297153711     0.8595890411     0.8595890411     0.6836734694     0.8307514191    
4                0.4758599699     0.3861646652     0.8620246649     0.9246575342     0.9246575342     0.5612244898     1.0373530388    
5                0.3685673177     0.4454052448     0.8139725924     0.8801369863     0.8801369863     0.6428571429     1.2407474518    
6                0.3496371508     0.3484378755     0.6980750561     0.9041095890     0.9041095890     0.5102040816     1.4961931705    
7                0.3801294267     0.4981172681     0.8782466650     0.9452054795     0.9452054795     0.5714285714     1.7093639374    
8                0.2839981914     0.2996377945     0.5836359859     0.8527397260     0.8527397260     0.6530612245     1.9135828018    
9                0.4413455427     0.5678585172     1.0092040300     0.9383561644     0.9383561644     0.6734693878     2.1379885674    

========ROUND 1========
====Feature update====
epoch            class_loss      
0                0.6554425359    
1                0.5289459229    
2                0.4155112803    
3                0.4305210412    
4                0.3911316097    
5                0.5918305516    
6                0.3714175224    
7                0.3148938417    
8                0.3202521503    
9                0.4740878940    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                0.6917374134     0.6245179176     0.0672194660    
1                0.7645683289     0.6743911505     0.0901771560    
2                0.5781175494     0.5266646147     0.0514529422    
3                0.6467104554     0.6006227732     0.0460876934    
4                0.6079596281     0.5781451464     0.0298145022    
5                0.6093708873     0.5729094744     0.0364614092    
6                0.6015005708     0.5670398474     0.0344607346    
7                0.5990124941     0.5664063692     0.0326061174    
8                0.6123722792     0.5829532146     0.0294190515    
9                0.6425149441     0.5944688916     0.0480460711    
Counter({0: 2928, 1: 2328})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.3278086185     0.3306751847     0.6584838033     0.9349315068     0.9349315068     0.4387755102     0.2119698524    
1                0.3221034408     0.2812350094     0.6033384800     0.7945205479     0.7945205479     0.3979591837     0.3948309422    
2                0.2603659332     0.4653501213     0.7257160544     0.8253424658     0.8253424658     0.4183673469     0.5729544163    
3                0.3047630489     0.2627264559     0.5674895048     0.8047945205     0.8047945205     0.4489795918     0.7835192680    
4                0.2443135828     0.1776745617     0.4219881296     0.9520547945     0.9520547945     0.4591836735     0.9555764198    
5                0.2976221144     0.2386110872     0.5362331867     0.9897260274     0.9897260274     0.5204081633     1.1255946159    
6                0.2473720014     0.4172085822     0.6645805836     0.9965753425     0.9965753425     0.6530612245     1.2955944538    
7                0.2513641417     0.1961684227     0.4475325644     0.9075342466     0.9075342466     0.6734693878     1.4629058838    
8                0.3009998798     0.3432030380     0.6442029476     0.8013698630     0.8013698630     0.6938775510     1.6529858112    
9                0.2314644754     0.2393811494     0.4708456397     1.0000000000     1.0000000000     0.6224489796     1.8600547314    

========ROUND 2========
====Feature update====
epoch            class_loss      
0                0.6704659462    
1                0.3151160479    
2                0.3334055543    
3                0.2958554029    
4                0.3058389723    
5                0.2499823570    
6                0.2547022998    
7                0.2360789627    
8                0.2917160988    
9                0.2005665600    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                0.6549090743     0.5696528554     0.0852562338    
1                0.5391003489     0.5091343522     0.0299659856    
2                0.5759558678     0.5458868742     0.0300690103    
3                0.6334229708     0.6125484705     0.0208744816    
4                0.6407468319     0.6077308059     0.0330160558    
5                0.5682428479     0.5178682804     0.0503745861    
6                0.6274527907     0.5743557215     0.0530970581    
7                0.5711581707     0.5530074835     0.0181506816    
8                0.6314443946     0.6001049280     0.0313394926    
9                0.5775972009     0.5243476629     0.0532495305    
Counter({0: 2957, 1: 2299})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.1971980035     0.3238292038     0.5210272074     0.7123287671     0.7123287671     0.7040816327     0.1725089550    
1                0.1311074793     0.2847535908     0.4158610702     0.9452054795     0.9452054795     0.6938775510     0.3165221214    
2                0.1426993012     0.4397027194     0.5824019909     0.9828767123     0.9828767123     0.6020408163     0.4867458344    
3                0.1365368515     0.3178834617     0.4544203281     0.9965753425     0.9965753425     0.5816326531     0.6307446957    
4                0.1671330482     0.3008703887     0.4680034518     0.9828767123     0.9828767123     0.6734693878     0.7997450829    
5                0.0976831391     0.4336726367     0.5313557982     0.9965753425     0.9965753425     0.6020408163     0.9997441769    
6                0.1394138783     0.4119080901     0.5513219833     0.9965753425     0.9965753425     0.6122448980     1.1657445431    
7                0.0967833176     0.2696374357     0.3664207458     1.0000000000     1.0000000000     0.6122448980     1.3167438507    
8                0.1544407159     0.4975722134     0.6520129442     0.9691780822     0.9691780822     0.6836734694     1.4687860012    
9                0.0989250690     0.2632631660     0.3621882200     0.9931506849     0.9931506849     0.6530612245     1.6427860260    

========ROUND 3========
====Feature update====
epoch            class_loss      
0                0.2884669602    
1                0.2254141867    
2                0.2591244876    
3                0.1358128488    
4                0.2634295821    
5                0.1716442853    
6                0.2038905323    
7                0.1870853901    
8                0.2098576576    
9                0.1532320529    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                0.6571928859     0.6253899336     0.0318029597    
1                0.5784321427     0.5389625430     0.0394695811    
2                0.5677192211     0.5404549837     0.0272642598    
3                0.6235909462     0.5911226273     0.0324682891    
4                0.5621308684     0.5361796618     0.0259512328    
5                0.6139868498     0.5862665772     0.0277202725    
6                0.5631445646     0.5306752324     0.0324693210    
7                0.6798731089     0.6384855509     0.0413875692    
8                0.5637830496     0.5353105664     0.0284724534    
9                0.6057268977     0.5689501762     0.0367767103    
Counter({0: 3086, 1: 2170})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.1318146884     0.4883880615     0.6202027798     1.0000000000     1.0000000000     0.6530612245     0.1619992256    
1                0.0894974694     0.2937124372     0.3832099140     1.0000000000     1.0000000000     0.5204081633     0.3179998398    
2                0.1042265967     0.2611876726     0.3654142618     0.9931506849     0.9931506849     0.5000000000     0.5050327778    
3                0.0760499537     0.5092814565     0.5853314400     0.9965753425     0.9965753425     0.6224489796     0.6510312557    
4                0.2666351795     0.3346880078     0.6013231874     0.8698630137     0.8698630137     0.6836734694     0.8020308018    
5                0.0583698787     0.3153221905     0.3736920655     0.9931506849     0.9931506849     0.5714285714     0.9500305653    
6                0.0613112450     0.3221064508     0.3834176958     0.9931506849     0.9931506849     0.6632653061     1.1110301018    
7                0.0983052552     0.3715983927     0.4699036479     0.9760273973     0.9760273973     0.6734693878     1.2740302086    
8                0.0639244914     0.4373510778     0.5012755394     0.9965753425     0.9965753425     0.5918367347     1.4300296307    
9                0.0588726066     0.3245190084     0.3833916187     0.9965753425     0.9965753425     0.6938775510     1.6280751228    

========ROUND 4========
====Feature update====
epoch            class_loss      
0                0.2340610176    
1                0.1550305039    
2                0.2563817799    
3                0.2017545402    
4                0.1735689789    
5                0.1567270756    
6                0.1366009116    
7                0.2956647277    
8                0.1744476259    
9                0.1260197461    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                0.5680701733     0.5257917047     0.0422784425    
1                0.5959444046     0.5793905854     0.0165537968    
2                0.5541849136     0.5362221003     0.0179627910    
3                0.5867801309     0.5708301067     0.0159500297    
4                0.5931161046     0.5496149063     0.0435012020    
5                0.6086736321     0.5852401257     0.0234334916    
6                0.5047160983     0.4745464027     0.0301697142    
7                0.5863865614     0.5723270774     0.0140595036    
8                0.5198187232     0.5027374029     0.0170813091    
9                0.6708908677     0.6494494081     0.0214414429    
Counter({0: 3016, 1: 2240})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.0806466043     0.2893928587     0.3700394630     0.9726027397     0.9726027397     0.7040816327     0.1709995270    
1                0.0708101392     0.4057961404     0.4766062796     1.0000000000     1.0000000000     0.5816326531     0.3159992695    
2                0.0401567407     0.3119914532     0.3521482050     0.9965753425     0.9965753425     0.6632653061     0.4679992199    
3                0.0642108768     0.3266140521     0.3908249140     0.9931506849     0.9931506849     0.6938775510     0.6140530109    
4                0.0891339555     0.4154984355     0.5046324134     0.9794520548     0.9794520548     0.5102040816     0.7760531902    
5                0.0375250876     0.3659404516     0.4034655392     1.0000000000     1.0000000000     0.7040816327     0.9660539627    
6                0.1900870949     0.3007119000     0.4907990098     0.8938356164     0.8938356164     0.7142857143     1.1080517769    
7                0.0398561731     0.3797211349     0.4195773005     0.9965753425     0.9965753425     0.5918367347     1.2410516739    
8                0.0850007832     0.3392687142     0.4242694974     0.9965753425     0.9965753425     0.6632653061     1.3950521946    
9                0.1136208251     0.3485057354     0.4621265531     1.0000000000     1.0000000000     0.6938775510     1.5671362877    

========ROUND 5========
====Feature update====
epoch            class_loss      
0                0.1903606355    
1                0.1416265666    
2                0.1512245536    
3                0.2252587676    
4                0.1298205703    
5                0.1189505830    
6                0.1018040776    
7                0.1115378365    
8                0.2341425866    
9                0.0920756534    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                0.5121697783     0.4575697482     0.0546000563    
1                0.5741518736     0.5546351671     0.0195167270    
2                0.4959532619     0.4816530347     0.0143002216    
3                0.6349867582     0.6176066399     0.0173801202    
4                0.5302964449     0.5179640651     0.0123323631    
5                0.5649609566     0.5503198504     0.0146411043    
6                0.5919200182     0.5239510536     0.0679689422    
7                0.5767954588     0.5632407665     0.0135546876    
8                0.5152424574     0.5064748526     0.0087675825    
9                0.5654075742     0.5575867891     0.0078208046    
Counter({0: 3003, 1: 2253})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.0621927567     0.3410046697     0.4031974375     1.0000000000     1.0000000000     0.6326530612     0.1609992981    
1                0.0466871113     0.5308911204     0.5775782466     1.0000000000     1.0000000000     0.6020408163     0.3299994469    
2                0.1964044273     0.4388748407     0.6352792978     0.8527397260     0.8527397260     0.7040816327     0.5673129559    
3                0.0564419888     0.1793096960     0.2357516885     1.0000000000     1.0000000000     0.6734693878     0.7493658066    
4                0.0244291779     0.5433238745     0.5677530766     1.0000000000     1.0000000000     0.6326530612     0.9363656044    
5                0.0664636344     0.3102377057     0.3767013550     1.0000000000     1.0000000000     0.6938775510     1.1153645515    
6                0.0302413534     0.3176578581     0.3478991985     0.9965753425     0.9965753425     0.6632653061     1.2793645859    
