当前的args.activity_id:1
还原的分组结果: {0: [2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 22.0, 23.0, 24.0, 25.0, 26.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 47.0, 48.0, 49.0, 50.0, 51.0, 53.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 79.0, 82.0, 83.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 98.0, 99.0, 100.0, 102.0, 105.0, 107.0, 109.0, 113.0, 115.0, 116.0, 119.0, 120.0, 122.0, 125.0, 127.0, 128.0], 1: [103.0, 104.0, 106.0, 110.0, 111.0, 112.0, 114.0, 117.0, 118.0, 123.0, 124.0, 126.0, 129.0, 130.0, 131.0, 133.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 149.0, 150.0, 151.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 161.0, 162.0, 163.0, 164.0, 166.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 194.0, 195.0, 197.0, 198.0, 201.0, 202.0, 206.0, 208.0, 210.0, 211.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 223.0, 226.0, 228.0, 233.0, 234.0, 235.0, 236.0, 237.0, 239.0, 240.0, 242.0, 243.0, 244.0, 245.0, 246.0, 248.0], 2: [196.0, 200.0, 207.0, 209.0, 213.0, 222.0, 224.0, 225.0, 238.0, 241.0, 249.0, 252.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 309.0, 310.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 322.0, 323.0, 324.0, 325.0, 327.0, 328.0, 331.0, 332.0, 333.0, 336.0, 338.0, 339.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0], 3: [321.0, 326.0, 330.0, 334.0, 340.0, 350.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 390.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 403.0, 404.0, 405.0, 406.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 417.0, 418.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 445.0, 446.0, 447.0, 448.0, 450.0, 451.0, 452.0, 453.0, 454.0, 456.0, 458.0, 460.0, 461.0, 462.0, 464.0, 465.0, 467.0, 468.0, 469.0]}
Environment:
	Python: 3.10.9
	PyTorch: 1.13.1
	Torchvision: 0.14.1
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.23.5
	PIL: 10.4.0
==========================================
algorithm:diversify
alpha:0.1
alpha1:1.0
batch_size:1152
beta1:0.5
bottleneck:256
checkpoint_freq:100
classifier:linear
data_file:
dataset:pads
data_dir:./data/
dis_hidden:256
gpu_id:0
layer:bn
lam:0.0
latent_domain_num:2
local_epoch:10
lr:0.01
lr_decay1:1.0
lr_decay2:1.0
max_epoch:100
model_size:median
N_WORKERS:4
old:False
seed:0
task:cross_people
test_envs:[0]
output:./data/train_output/act/cross_people-our-Diversify-0-10-1-1-0-3-50-0.01
weight_decay:0.0005
activity_id:1
steps_per_epoch:10000000000
select_position:{'pads': [0]}
select_channel:{'pads': array([0, 1, 2, 3, 4, 5])}
hz_list:{'pads': 100}
act_people:{'pads': [[2.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 22.0, 23.0, 24.0, 25.0, 26.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 47.0, 48.0, 49.0, 50.0, 51.0, 53.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 79.0, 82.0, 83.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 98.0, 99.0, 100.0, 102.0, 105.0, 107.0, 109.0, 113.0, 115.0, 116.0, 119.0, 120.0, 122.0, 125.0, 127.0, 128.0], [103.0, 104.0, 106.0, 110.0, 111.0, 112.0, 114.0, 117.0, 118.0, 123.0, 124.0, 126.0, 129.0, 130.0, 131.0, 133.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 149.0, 150.0, 151.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 161.0, 162.0, 163.0, 164.0, 166.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 194.0, 195.0, 197.0, 198.0, 201.0, 202.0, 206.0, 208.0, 210.0, 211.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 223.0, 226.0, 228.0, 233.0, 234.0, 235.0, 236.0, 237.0, 239.0, 240.0, 242.0, 243.0, 244.0, 245.0, 246.0, 248.0], [196.0, 200.0, 207.0, 209.0, 213.0, 222.0, 224.0, 225.0, 238.0, 241.0, 249.0, 252.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 273.0, 274.0, 275.0, 276.0, 277.0, 278.0, 279.0, 280.0, 281.0, 282.0, 286.0, 287.0, 288.0, 289.0, 290.0, 291.0, 292.0, 293.0, 296.0, 297.0, 298.0, 299.0, 300.0, 301.0, 302.0, 303.0, 304.0, 305.0, 306.0, 307.0, 309.0, 310.0, 312.0, 313.0, 314.0, 315.0, 316.0, 317.0, 318.0, 322.0, 323.0, 324.0, 325.0, 327.0, 328.0, 331.0, 332.0, 333.0, 336.0, 338.0, 339.0, 341.0, 342.0, 343.0, 344.0, 345.0, 346.0, 347.0, 348.0, 349.0, 351.0, 352.0, 353.0, 354.0, 355.0, 356.0, 357.0, 358.0], [321.0, 326.0, 330.0, 334.0, 340.0, 350.0, 359.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 366.0, 367.0, 368.0, 369.0, 372.0, 373.0, 374.0, 375.0, 376.0, 377.0, 378.0, 380.0, 381.0, 382.0, 383.0, 384.0, 385.0, 386.0, 387.0, 388.0, 390.0, 392.0, 393.0, 394.0, 395.0, 396.0, 397.0, 398.0, 399.0, 400.0, 401.0, 403.0, 404.0, 405.0, 406.0, 409.0, 410.0, 411.0, 412.0, 413.0, 414.0, 415.0, 417.0, 418.0, 421.0, 422.0, 423.0, 424.0, 425.0, 426.0, 428.0, 429.0, 430.0, 431.0, 432.0, 433.0, 434.0, 435.0, 437.0, 438.0, 439.0, 440.0, 441.0, 442.0, 443.0, 445.0, 446.0, 447.0, 448.0, 450.0, 451.0, 452.0, 453.0, 454.0, 456.0, 458.0, 460.0, 461.0, 462.0, 464.0, 465.0, 467.0, 468.0, 469.0]]}
num_classes:2
input_shape:(6, 1, 100)
grid_size:10


========ROUND 0========
====Feature update====
epoch            class_loss      
0                1.4057345390    
1                1.0655423403    
2                0.9486770630    
3                0.6605066657    
4                0.5273874402    
5                0.6355366707    
6                0.4503616989    
7                0.4785713255    
8                0.4503014982    
9                0.4406233430    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                1.1305075884     0.4906949699     0.6398125887    
1                0.8935236335     0.4803805351     0.4131430984    
2                0.7795544267     0.5640267134     0.2155277133    
3                0.6258750558     0.5437644124     0.0821106285    
4                0.6805156469     0.6468704939     0.0336451530    
5                0.6499156952     0.6330817342     0.0168339610    
6                0.5365858674     0.5270144343     0.0095714470    
7                0.5661242604     0.5588772297     0.0072470447    
8                0.6009868383     0.5968801379     0.0041067000    
9                0.5314094424     0.5288978815     0.0025115449    
Counter({0: 3081, 1: 2175})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.8741043210     0.3334515989     1.2075558901     0.7465753425     0.7465753425     0.4693877551     0.2210001945    
1                0.4418708384     0.2407872975     0.6826581359     0.8184931507     0.8184931507     0.6428571429     0.3729999065    
2                0.3595815301     0.2634386122     0.6230201721     0.8630136986     0.8630136986     0.6530612245     0.5185384750    
3                0.4453960955     0.3843193054     0.8297153711     0.8595890411     0.8595890411     0.6836734694     0.6685378551    
4                0.4758599699     0.3861646652     0.8620246649     0.9246575342     0.9246575342     0.5612244898     0.8245379925    
5                0.3685673177     0.4454052448     0.8139725924     0.8801369863     0.8801369863     0.6428571429     0.9715375900    
6                0.3496371508     0.3484378755     0.6980750561     0.9041095890     0.9041095890     0.5102040816     1.1635372639    
7                0.3801294267     0.4981172681     0.8782466650     0.9452054795     0.9452054795     0.5714285714     1.3125388622    
8                0.2839981914     0.2996377945     0.5836359859     0.8527397260     0.8527397260     0.6530612245     1.4676079750    
9                0.4413455427     0.5678585172     1.0092040300     0.9383561644     0.9383561644     0.6734693878     1.6306087971    

========ROUND 1========
====Feature update====
epoch            class_loss      
0                0.6554425359    
1                0.5289459229    
2                0.4155112803    
3                0.4305210412    
4                0.3911316097    
5                0.5918305516    
6                0.3714175224    
7                0.3148938417    
8                0.3202521503    
9                0.4740878940    
====Latent domain characterization====
epoch            total_loss       dis_loss         ent_loss        
0                0.6917374134     0.6245179176     0.0672194660    
1                0.7645683289     0.6743911505     0.0901771560    
2                0.5781175494     0.5266646147     0.0514529422    
3                0.6467104554     0.6006227732     0.0460876934    
4                0.6079596281     0.5781451464     0.0298145022    
5                0.6093708873     0.5729094744     0.0364614092    
6                0.6015005708     0.5670398474     0.0344607346    
7                0.5990124941     0.5664063692     0.0326061174    
8                0.6123722792     0.5829532146     0.0294190515    
9                0.6425149441     0.5944688916     0.0480460711    
Counter({0: 2928, 1: 2328})
====Domain-invariant feature learning====
epoch            class_loss       dis_loss         total_loss       train_acc        valid_acc        target_acc       total_cost_time 
0                0.3278086185     0.3306751847     0.6584838033     0.9349315068     0.9349315068     0.4387755102     0.1790001392    
1                0.3221034408     0.2812350094     0.6033384800     0.7945205479     0.7945205479     0.3979591837     0.3439996243    
2                0.2603659332     0.4653501213     0.7257160544     0.8253424658     0.8253424658     0.4183673469     0.5080535412    
3                0.3047630489     0.2627264559     0.5674895048     0.8047945205     0.8047945205     0.4489795918     0.7118842602    
